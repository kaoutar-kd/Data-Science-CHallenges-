{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287e3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa \n",
    "import librosa.display\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Flatten\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "dftrain = pd.read_csv('traintwd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc2503db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audiofiles(path):\n",
    "    data = np.zeros((145,1025,86))\n",
    "    hop_length = 512 # in num. of samples\n",
    "    n_fft = 2048\n",
    "    audionames = os.listdir(path)\n",
    "    for audio in audionames[1:]: \n",
    "        file = os.path.join(path, audio)\n",
    "        signal, sample_rate = librosa.load(file, duration = 2.00) \n",
    "        stft = librosa.stft(signal, n_fft=n_fft, hop_length=hop_length)\n",
    "        spectrogram = np.abs(stft)\n",
    "        log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "        MFCCs = librosa.feature.mfcc(signal, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=40)\n",
    "        \n",
    "        s = librosa.util.fix_length(spectrogram,86)\n",
    "        data[audionames.index(audio)] = s  \n",
    "    return data\n",
    "X = process_audiofiles(\"TWDAUDIOS\")[:144]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9385871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohe(df,path):\n",
    "    ids=[]\n",
    "    for d in os.listdir(path)[1:]:\n",
    "        ids.append(d.removesuffix('.wav'))\n",
    "    df1 = pd.DataFrame({'id' : ids})\n",
    "    dffinal = df1.merge(df, on ='id')\n",
    "    Y = dffinal['label']\n",
    "    Y= to_categorical(Y)\n",
    "    return Y \n",
    "Y=cohe(dftrain,\"TWDAUDIOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca0acff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc1b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelff(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = tf.keras.Input(shape = input_shape)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: CONV layer (≈4 lines)\n",
    "    # Add a Conv1D with 196 units, kernel size of 15 and stride of 4\n",
    "    X = Conv1D(filters=196, kernel_size=15, strides=4)(X_input)\n",
    "    # Batch normalization\n",
    "    X = BatchNormalization()(X)\n",
    "    # ReLu activation\n",
    "    X = Activation(\"relu\")(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(rate=0.8)(X)                                 \n",
    "\n",
    "    # Step 2: First GRU Layer (≈4 lines)\n",
    "    # GRU (use 128 units and return the sequences)\n",
    "    X = GRU(units=128, return_sequences=True)(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    # Batch normalization.\n",
    "    X = BatchNormalization()(X)                           \n",
    "    \n",
    "    # Step 3: Second GRU Layer (≈4 lines)\n",
    "    # GRU (use 128 units and return the sequences)\n",
    "    X = GRU(units=128, return_sequences=True)(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(rate=0.8)(X)\n",
    "    # Batch normalization\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "                            \n",
    "\n",
    "    # Step 4: Time-distributed dense layer (≈1 line)\n",
    "    # TimeDistributed  with sigmoid activation \n",
    "    X= Flatten()(X)\n",
    "\n",
    "    X=Dense(2, activation=\"sigmoid\")(X)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7179b85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3501, 86)]        0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 872, 196)          253036    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 872, 196)         784       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 872, 196)          0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 872, 196)          0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 872, 128)          125184    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 872, 128)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 872, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 872, 128)          99072     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 872, 128)          0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 872, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 111616)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 223234    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 702,334\n",
      "Trainable params: 701,430\n",
      "Non-trainable params: 904\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= modelff((1025, 86)) \n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer= 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "531c80df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 49s 674ms/step - loss: 5.2334 - accuracy: 0.4931\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 48s 667ms/step - loss: 4.6935 - accuracy: 0.5556\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 48s 664ms/step - loss: 5.0038 - accuracy: 0.5417\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 48s 666ms/step - loss: 5.3297 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 48s 666ms/step - loss: 5.7202 - accuracy: 0.5486\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 48s 664ms/step - loss: 6.1269 - accuracy: 0.5625\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 47s 657ms/step - loss: 5.9331 - accuracy: 0.5139\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 47s 660ms/step - loss: 6.1137 - accuracy: 0.6250\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 47s 659ms/step - loss: 6.3292 - accuracy: 0.5694\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 48s 660ms/step - loss: 7.7683 - accuracy: 0.5208\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 48s 667ms/step - loss: 6.8703 - accuracy: 0.5417\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 48s 666ms/step - loss: 5.1744 - accuracy: 0.5972\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 48s 667ms/step - loss: 6.7482 - accuracy: 0.5694\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 47s 658ms/step - loss: 7.1365 - accuracy: 0.5764\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 47s 657ms/step - loss: 7.2411 - accuracy: 0.5208\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 48s 663ms/step - loss: 5.9357 - accuracy: 0.5833\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 48s 661ms/step - loss: 5.8342 - accuracy: 0.6597\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 47s 659ms/step - loss: 6.3440 - accuracy: 0.6528\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 47s 660ms/step - loss: 6.0665 - accuracy: 0.6389\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 47s 656ms/step - loss: 5.8564 - accuracy: 0.6458\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 47s 658ms/step - loss: 6.7226 - accuracy: 0.5833\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 47s 656ms/step - loss: 7.6297 - accuracy: 0.6042\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 47s 654ms/step - loss: 6.0817 - accuracy: 0.6250\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 47s 656ms/step - loss: 5.0367 - accuracy: 0.7292\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 48s 660ms/step - loss: 6.8851 - accuracy: 0.6528\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 48s 666ms/step - loss: 5.7584 - accuracy: 0.6667\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 48s 662ms/step - loss: 6.9251 - accuracy: 0.6736\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 47s 655ms/step - loss: 6.2660 - accuracy: 0.7083\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 48s 661ms/step - loss: 6.8302 - accuracy: 0.6111\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 47s 652ms/step - loss: 7.1643 - accuracy: 0.6458\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 47s 655ms/step - loss: 5.7361 - accuracy: 0.6736\n",
      "Epoch 32/50\n",
      "72/72 [==============================] - 48s 662ms/step - loss: 7.3691 - accuracy: 0.6111\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 47s 660ms/step - loss: 6.6223 - accuracy: 0.6458\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 48s 660ms/step - loss: 7.0630 - accuracy: 0.6458\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 48s 669ms/step - loss: 5.7160 - accuracy: 0.6875\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 47s 658ms/step - loss: 5.8195 - accuracy: 0.6806\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 48s 661ms/step - loss: 8.4786 - accuracy: 0.5903\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 48s 666ms/step - loss: 6.3450 - accuracy: 0.6806\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 47s 659ms/step - loss: 5.5892 - accuracy: 0.7014\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 48s 664ms/step - loss: 5.4358 - accuracy: 0.7222\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 36s 497ms/step - loss: 5.6934 - accuracy: 0.6667\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 27s 380ms/step - loss: 7.1763 - accuracy: 0.6597\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 7.7451 - accuracy: 0.7014\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 29s 399ms/step - loss: 5.1855 - accuracy: 0.7569\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 25s 342ms/step - loss: 4.6624 - accuracy: 0.7083\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 25s 343ms/step - loss: 5.3751 - accuracy: 0.7292\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 24s 334ms/step - loss: 5.6443 - accuracy: 0.6667\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 24s 339ms/step - loss: 5.2230 - accuracy: 0.7014\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 24s 335ms/step - loss: 4.7631 - accuracy: 0.7500\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 24s 334ms/step - loss: 6.0081 - accuracy: 0.6736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cf3c56c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs = 50, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a78c0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "72/72 [==============================] - 25s 353ms/step - loss: 5.7397 - accuracy: 0.7292\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 26s 360ms/step - loss: 5.6087 - accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 24s 333ms/step - loss: 5.5246 - accuracy: 0.7222\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 24s 331ms/step - loss: 5.9463 - accuracy: 0.7292\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 24s 336ms/step - loss: 5.1786 - accuracy: 0.7014\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 24s 336ms/step - loss: 5.9274 - accuracy: 0.7361\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 24s 339ms/step - loss: 5.3809 - accuracy: 0.7361\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 5.1713 - accuracy: 0.7361\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 25s 353ms/step - loss: 6.1081 - accuracy: 0.7222\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 24s 333ms/step - loss: 5.3825 - accuracy: 0.7361\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23c8a69b2b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs = 10, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "542b7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xtest = process_audiofiles(\"TWDtest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dfd3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(Xtest[:82])\n",
    "res = np.argmax(res, axis = 1)\n",
    "df1 = pd.read_csv('testtwd.csv')\n",
    "df1['label'] = res\n",
    "df1.to_csv('twd_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317cec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
